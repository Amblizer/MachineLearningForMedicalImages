{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple binary classification problem utilizing convolutional neural networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 750 Ti (CNMeM is disabled, cuDNN 5105)\n",
      "/usr/local/lib/python3.5/dist-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries. \n",
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "# os.environ['THEANO_FLAGS']='mode=FAST_RUN,device=gpu0,floatX=float32,optimizer=fast_compile'\n",
    "# os.environ['KERAS_BACKEND'] = 'theano'\n",
    "\"\"\"\n",
    "os.environ['THEANO_FLAGS']='mode=FAST_RUN,device=gpu3,floatX=float32,optimizer=fast_compile'\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "\n",
    "In case you want to select a graphic card (i the above code i set the 3rd graphic card.) \n",
    "\"\"\"\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import numpy as np\n",
    "import keras \n",
    "import keras.backend as K\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "from keras import callbacks\n",
    "import glob\n",
    "from PIL import Image\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.utils.visualize_util import plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23582\n"
     ]
    }
   ],
   "source": [
    "# It is good to know the pid of the running code in case you need to stop  or monitor. \n",
    "print (os.getpid())\n",
    "file_open = lambda x,y: glob.glob(os.path.join(x,y))\n",
    "\n",
    "# learning rate schedule. It is helpful when the learning rate can be dynamically set up. We will be using the callback functionality that keras provides. \n",
    "def step_decay(epoch):\n",
    "\tinitial_lrate =0.01\n",
    "\tdrop = 0.3\n",
    "\tepochs_drop = 30.0\n",
    "\tlrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "\tprint (lrate)\n",
    "\treturn lrate\n",
    "\n",
    "# The following function will be used to give a number of the parameters in our model. Useful when we need to get an estimate of what size of dataset we have to use.  \n",
    "def size(model): \n",
    "\treturn sum([np.prod(K.get_value(w).shape) for w in model.trainable_weights])\n",
    "\n",
    "def createmodel(img_channels,img_rows,img_cols):\n",
    "\t# This is a Sequential model. Graph models can be used in order to create more complex networks. \n",
    "\t# Teaching Points:\n",
    "\t# 1. Here we utilize the adam optimization algorithm. In order to use the SGD algorithm one could replace the {adam=keras.optimizers.Adadelta(lr=0)} line with  {sgd = SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False)} make sure you import the correct optimizer from keras. \n",
    "\t# 2. This is a binary classification problem so make sure that the correct activation loss function combination is used. For such a problem the sigmoid activation function with the binary cross entropy loss is a good option\n",
    "\t# 3. Since this is a binary problem use \tmodel.add(Dense(1)) NOT 2...\n",
    "\t# 4. For multi class model this code can be easily modified by selecting the softmax as activation function and the categorical cross entropy as loss \n",
    "\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Convolution2D(16, 3, 3, border_mode='same',input_shape=(1, img_rows, img_cols)))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(Convolution2D(16, 5, 5, border_mode='same'))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Convolution2D(32, 3, 3, border_mode='same'))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(Convolution2D(64, 5, 5, border_mode='same'))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Convolution2D(128, 3, 3, border_mode='same'))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, init='he_normal'))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(Dropout(0.5)) \n",
    "\tmodel.add(Dense(32, init='he_normal'))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(Dropout(0.5)) \n",
    "\tmodel.add(Dense(1))\n",
    "\t# model.add(Activation('relu'))\n",
    "\tmodel.add(Activation('sigmoid'))\n",
    "\t# learning schedule callback\n",
    "\tadam=keras.optimizers.Adadelta(lr=0)\n",
    "\tlrate = LearningRateScheduler(step_decay)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\tplot(model, to_file='model.png')\n",
    "\treturn model\n",
    "\n",
    "def shuffle(X, y):\n",
    "\tperm = np.random.permutation(len(y))\n",
    "\tX = X[perm]\n",
    "\ty = y[perm]\n",
    "\tprint (np.shape(X))\n",
    "\treturn X, y\n",
    "\n",
    "\n",
    "def read_data(image):\n",
    "\t\"opens image and converts it to a m*n matrix\" \n",
    "\timage = Image.open(image)\n",
    "\timage = image.getdata()\n",
    "\t# image = list(image.getdata())\n",
    "\t# image = map(list,image)\n",
    "\timage = np.array(image)\n",
    "\treturn image.reshape(-1)\n",
    "\n",
    "def createTrainTestValset(image_dir1, image_dir2):\n",
    "\tClass1_images = file_open(image_dir1,\"*.jpg\")\n",
    "\tClass2_images = file_open(image_dir2,\"*.jpg\")\n",
    "\tClass1_set = []\n",
    "\tClass2_set = []\n",
    "\t# Read all the files, and create numpy arrays. \n",
    "\tClass1_set = [read_data(image) for image in Class1_images]\n",
    "\tClass2_set = [read_data(image) for image in Class2_images]\n",
    "\tClass1_set = np.array(Class1_set) #This is where the Memory Error occurs\n",
    "\tClass2_set = np.array(Class2_set)\n",
    "\tX=np.vstack((Class1_set, Class2_set))\n",
    "\tX=X.astype(np.float)/255\n",
    "\t# print (np.shape(X))\n",
    "\tyclass1=np.zeros((np.shape(Class1_set)[0]))\n",
    "\tyclass2=np.ones((np.shape(Class2_set)[0]))\n",
    "\t# print (np.shape(yclass1))\n",
    "\ty=np.concatenate((yclass1, yclass2))\n",
    "\t# print (np.shape(y))\t\n",
    "\tX,y=shuffle(X, y)\n",
    "\tprint (np.shape(X))\t\n",
    "\tprint (np.max(X))\n",
    "\tprint (np.shape(y))\t\n",
    "\tX_train, X_val,y_train, y_val= train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "\treturn X_train,y_train, X_val, y_val \n",
    "\n",
    "\t# Read the images; and split them in three different sets. \n",
    "def trainandpredict(Scan=32 ,img_channels=1,batch_size=64,nb_epoch=100,data_augmentation=False):\n",
    "\timg_rows=Scan\n",
    "\timg_cols=Scan\n",
    "\tCurrentDir= os.getcwd()\n",
    "\timage_dir1=os.path.abspath(os.path.join(os.path.abspath(os.path.join(CurrentDir, os.pardir)), \"Data\",\"negative_images\"))\n",
    "# \tprint(image_dir1)\n",
    "\timage_dir2=os.path.abspath(os.path.join(os.path.abspath(os.path.join(CurrentDir, os.pardir)), \"Data\",\"positive_images\"))\n",
    "\tmodeleval=createmodel(img_channels,img_rows,img_cols)\n",
    "\tX_train,y_train, X_val, y_val = createTrainTestValset(image_dir1, image_dir2)\n",
    "\tX_train =X_train.reshape(\n",
    "\t\t-1,  # number of samples, -1 makes it so that this number is determined automatically\n",
    "\t\t1,   # 1 color channel, since images are only black and white\n",
    "\t\tScan,  # first image dimension (vertical)\n",
    "\t\tScan,  # second image dimension (horizontal)\n",
    "\t)\n",
    "\tX_val =X_val.reshape(\n",
    "\t\t-1,  # number of samples, -1 makes it so that this number is determined automatically\n",
    "\t\t1,   # 1 color channel, since images are only black and white\n",
    "\t\tScan,  # first image dimension (vertical)\n",
    "\t\tScan,  # second image dimension (horizontal)\n",
    "\t)\n",
    "\t# Callbacks\n",
    "\tbest_model = ModelCheckpoint('Final{epoch:03d}_{val_acc:.2f}.h5', verbose=1, monitor='val_loss',save_best_only=True)\n",
    "\tlrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "\t# Data augmentation is always a good choice\n",
    "\tif not data_augmentation:\n",
    "\t\tprint('Not using data augmentation.')\n",
    "\t\tmodeleval.fit(X_train, y_train,batch_size=batch_size,nb_epoch=nb_epoch,validation_split=0.1,callbacks=[best_model,lrate],shuffle=True)\n",
    "\telse:\n",
    "\t\tprint('Using real-time data augmentation.')\n",
    "\t\tprint (\"pending\")\n",
    "\t\tsys.exit()\n",
    "\t\t# this will do preprocessing and realtime data augmentation\n",
    "\t\t# datagen = ImageDataGenerator(\n",
    "\t\t# \tfeaturewise_center=False,  # set input mean to 0 over the dataset\n",
    "\t\t# \tsamplewise_center=False,  # set each sample mean to 0\n",
    "\t\t# \tfeaturewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "\t\t# \tsamplewise_std_normalization=False,  # divide each input by its std\n",
    "\t\t# \tzca_whitening=False,  # apply ZCA whitening\n",
    "\t\t# \trotation_range=3,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "\t\t# \twidth_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "\t\t# \theight_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "\t\t# \thorizontal_flip=True,  # randomly flip images\n",
    "\t\t# \tvertical_flip=False)  # randomly flip images\n",
    "\t\t# modeleval.fit(X_train, y_train,batch_size=batch_size,nb_epoch=nb_epoch,validation_data=(X_train1, y_train1),callbacks=[best_model,lrate],shuffle=True)\n",
    "\n",
    "\n",
    "\t# Some evaluation Just the basic stuff... \n",
    "\tprint (dir(modeleval))\n",
    "\tY_cv_pred = modeleval.predict(X_val, batch_size = 32)\n",
    "\troc =roc_auc_score(y_val, Y_cv_pred)\n",
    "\tprint(\"ROC:\", roc)\n",
    "\tprint (Y_cv_pred)\n",
    "\tY_cv_pred[Y_cv_pred>=.5]=1\n",
    "\tY_cv_pred[Y_cv_pred<.5]=0\n",
    "\ttarget_names=[] \n",
    "\t# print (\"The f1-score gives you the harmonic mean of precision and recall. The scores corresponding to every class will tell you the accuracy of the classifier in classifying the data points in that particular class compared to all other classes.The support is the number of samples of the true response that lie in that class.\")\n",
    "\ttarget_names = ['class 0', 'class 1']\n",
    "\tprint(classification_report(y_val, Y_cv_pred, target_names=target_names,digits=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Program "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8710, 1024)\n",
      "(8710, 1024)\n",
      "1.0\n",
      "(8710,)\n",
      "Not using data augmentation.\n",
      "Train on 6271 samples, validate on 697 samples\n",
      "0.01\n",
      "Epoch 1/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.9123 - acc: 0.4779Epoch 00000: val_loss improved from inf to 0.68254, saving model to Final000_0.69.h5\n",
      "6271/6271 [==============================] - 27s - loss: 0.9101 - acc: 0.4784 - val_loss: 0.6825 - val_acc: 0.6944\n",
      "0.01\n",
      "Epoch 2/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.7078 - acc: 0.5677Epoch 00001: val_loss improved from 0.68254 to 0.66135, saving model to Final001_0.69.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.7074 - acc: 0.5683 - val_loss: 0.6614 - val_acc: 0.6944\n",
      "0.01\n",
      "Epoch 3/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.6618 - acc: 0.6203Epoch 00002: val_loss improved from 0.66135 to 0.62682, saving model to Final002_0.69.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.6614 - acc: 0.6208 - val_loss: 0.6268 - val_acc: 0.6944\n",
      "0.01\n",
      "Epoch 4/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.6372 - acc: 0.6392Epoch 00003: val_loss improved from 0.62682 to 0.59426, saving model to Final003_0.69.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.6361 - acc: 0.6395 - val_loss: 0.5943 - val_acc: 0.6944\n",
      "0.01\n",
      "Epoch 5/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.6314 - acc: 0.6471Epoch 00004: val_loss improved from 0.59426 to 0.57964, saving model to Final004_0.69.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.6312 - acc: 0.6471 - val_loss: 0.5796 - val_acc: 0.6944\n",
      "0.01\n",
      "Epoch 6/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.6154 - acc: 0.6540Epoch 00005: val_loss improved from 0.57964 to 0.56744, saving model to Final005_0.69.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.6154 - acc: 0.6548 - val_loss: 0.5674 - val_acc: 0.6944\n",
      "0.01\n",
      "Epoch 7/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.6059 - acc: 0.6685Epoch 00006: val_loss improved from 0.56744 to 0.55579, saving model to Final006_0.69.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.6060 - acc: 0.6680 - val_loss: 0.5558 - val_acc: 0.6944\n",
      "0.01\n",
      "Epoch 8/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.5987 - acc: 0.6703Epoch 00007: val_loss improved from 0.55579 to 0.55019, saving model to Final007_0.69.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.5986 - acc: 0.6701 - val_loss: 0.5502 - val_acc: 0.6944\n",
      "0.01\n",
      "Epoch 9/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.5886 - acc: 0.6772Epoch 00008: val_loss improved from 0.55019 to 0.54543, saving model to Final008_0.69.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.5879 - acc: 0.6777 - val_loss: 0.5454 - val_acc: 0.6944\n",
      "0.01\n",
      "Epoch 10/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.5823 - acc: 0.6794Epoch 00009: val_loss improved from 0.54543 to 0.53700, saving model to Final009_0.69.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.5822 - acc: 0.6793 - val_loss: 0.5370 - val_acc: 0.6944\n",
      "0.01\n",
      "Epoch 11/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.5817 - acc: 0.6835Epoch 00010: val_loss improved from 0.53700 to 0.53214, saving model to Final010_0.69.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.5816 - acc: 0.6833 - val_loss: 0.5321 - val_acc: 0.6944\n",
      "0.01\n",
      "Epoch 12/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.5672 - acc: 0.6891Epoch 00011: val_loss improved from 0.53214 to 0.52523, saving model to Final011_0.70.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.5665 - acc: 0.6898 - val_loss: 0.5252 - val_acc: 0.6958\n",
      "0.01\n",
      "Epoch 13/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.5677 - acc: 0.6930Epoch 00012: val_loss improved from 0.52523 to 0.51948, saving model to Final012_0.70.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.5671 - acc: 0.6935 - val_loss: 0.5195 - val_acc: 0.6958\n",
      "0.01\n",
      "Epoch 14/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.5542 - acc: 0.6941Epoch 00013: val_loss improved from 0.51948 to 0.51303, saving model to Final013_0.70.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.5539 - acc: 0.6945 - val_loss: 0.5130 - val_acc: 0.6973\n",
      "0.01\n",
      "Epoch 15/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.5526 - acc: 0.6967Epoch 00014: val_loss improved from 0.51303 to 0.50635, saving model to Final014_0.70.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.5522 - acc: 0.6969 - val_loss: 0.5063 - val_acc: 0.6958\n",
      "0.01\n",
      "Epoch 16/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.5474 - acc: 0.7007Epoch 00015: val_loss improved from 0.50635 to 0.50080, saving model to Final015_0.70.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.5474 - acc: 0.7004 - val_loss: 0.5008 - val_acc: 0.6987\n",
      "0.01\n",
      "Epoch 17/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.5332 - acc: 0.7081Epoch 00016: val_loss improved from 0.50080 to 0.49365, saving model to Final016_0.70.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.5335 - acc: 0.7082 - val_loss: 0.4937 - val_acc: 0.6958\n",
      "0.01\n",
      "Epoch 18/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.5363 - acc: 0.7096Epoch 00017: val_loss improved from 0.49365 to 0.48729, saving model to Final017_0.70.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.5357 - acc: 0.7099 - val_loss: 0.4873 - val_acc: 0.6958\n",
      "0.01\n",
      "Epoch 19/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.5322 - acc: 0.7101Epoch 00018: val_loss improved from 0.48729 to 0.47983, saving model to Final018_0.71.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.5312 - acc: 0.7107 - val_loss: 0.4798 - val_acc: 0.7059\n",
      "0.01\n",
      "Epoch 20/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.5174 - acc: 0.7194Epoch 00019: val_loss improved from 0.47983 to 0.47323, saving model to Final019_0.71.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.5172 - acc: 0.7195 - val_loss: 0.4732 - val_acc: 0.7131\n",
      "0.01\n",
      "Epoch 21/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.5162 - acc: 0.7213Epoch 00020: val_loss improved from 0.47323 to 0.46617, saving model to Final020_0.73.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.5176 - acc: 0.7214 - val_loss: 0.4662 - val_acc: 0.7317\n",
      "0.01\n",
      "Epoch 22/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.5228 - acc: 0.7223Epoch 00021: val_loss improved from 0.46617 to 0.46253, saving model to Final021_0.72.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.5227 - acc: 0.7213 - val_loss: 0.4625 - val_acc: 0.7231\n",
      "0.01\n",
      "Epoch 23/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.5023 - acc: 0.7329Epoch 00022: val_loss improved from 0.46253 to 0.45707, saving model to Final022_0.74.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.5022 - acc: 0.7334 - val_loss: 0.4571 - val_acc: 0.7389\n",
      "0.01\n",
      "Epoch 24/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4999 - acc: 0.7374Epoch 00023: val_loss improved from 0.45707 to 0.44773, saving model to Final023_0.74.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4994 - acc: 0.7377 - val_loss: 0.4477 - val_acc: 0.7432\n",
      "0.01\n",
      "Epoch 25/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.5030 - acc: 0.7331Epoch 00024: val_loss improved from 0.44773 to 0.44206, saving model to Final024_0.75.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.5032 - acc: 0.7331 - val_loss: 0.4421 - val_acc: 0.7532\n",
      "0.01\n",
      "Epoch 26/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4917 - acc: 0.7410Epoch 00025: val_loss improved from 0.44206 to 0.43643, saving model to Final025_0.76.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4913 - acc: 0.7423 - val_loss: 0.4364 - val_acc: 0.7590\n",
      "0.01\n",
      "Epoch 27/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4822 - acc: 0.7490Epoch 00026: val_loss improved from 0.43643 to 0.43050, saving model to Final026_0.76.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4816 - acc: 0.7490 - val_loss: 0.4305 - val_acc: 0.7618\n",
      "0.01\n",
      "Epoch 28/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4829 - acc: 0.7399Epoch 00027: val_loss improved from 0.43050 to 0.42545, saving model to Final027_0.77.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4821 - acc: 0.7409 - val_loss: 0.4255 - val_acc: 0.7676\n",
      "0.01\n",
      "Epoch 29/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4763 - acc: 0.7468Epoch 00028: val_loss improved from 0.42545 to 0.42169, saving model to Final028_0.76.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4754 - acc: 0.7474 - val_loss: 0.4217 - val_acc: 0.7647\n",
      "0.003\n",
      "Epoch 30/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4735 - acc: 0.7526Epoch 00029: val_loss improved from 0.42169 to 0.41781, saving model to Final029_0.78.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4731 - acc: 0.7524 - val_loss: 0.4178 - val_acc: 0.7776\n",
      "0.003\n",
      "Epoch 31/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4661 - acc: 0.7613Epoch 00030: val_loss improved from 0.41781 to 0.41600, saving model to Final030_0.78.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4661 - acc: 0.7611 - val_loss: 0.4160 - val_acc: 0.7819\n",
      "0.003\n",
      "Epoch 32/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4702 - acc: 0.7527Epoch 00031: val_loss improved from 0.41600 to 0.41430, saving model to Final031_0.78.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4710 - acc: 0.7522 - val_loss: 0.4143 - val_acc: 0.7834\n",
      "0.003\n",
      "Epoch 33/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4622 - acc: 0.7650Epoch 00032: val_loss improved from 0.41430 to 0.41279, saving model to Final032_0.78.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4624 - acc: 0.7651 - val_loss: 0.4128 - val_acc: 0.7834\n",
      "0.003\n",
      "Epoch 34/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4653 - acc: 0.7574Epoch 00033: val_loss improved from 0.41279 to 0.41211, saving model to Final033_0.78.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4656 - acc: 0.7571 - val_loss: 0.4121 - val_acc: 0.7819\n",
      "0.003\n",
      "Epoch 35/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4567 - acc: 0.7655Epoch 00034: val_loss improved from 0.41211 to 0.41031, saving model to Final034_0.78.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4568 - acc: 0.7648 - val_loss: 0.4103 - val_acc: 0.7834\n",
      "0.003\n",
      "Epoch 36/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4612 - acc: 0.7661Epoch 00035: val_loss improved from 0.41031 to 0.40951, saving model to Final035_0.78.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4611 - acc: 0.7657 - val_loss: 0.4095 - val_acc: 0.7848\n",
      "0.003\n",
      "Epoch 37/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4652 - acc: 0.7597Epoch 00036: val_loss improved from 0.40951 to 0.40762, saving model to Final036_0.79.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4645 - acc: 0.7605 - val_loss: 0.4076 - val_acc: 0.7862\n",
      "0.003\n",
      "Epoch 38/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4573 - acc: 0.7632Epoch 00037: val_loss improved from 0.40762 to 0.40578, saving model to Final037_0.78.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4577 - acc: 0.7630 - val_loss: 0.4058 - val_acc: 0.7819\n",
      "0.003\n",
      "Epoch 39/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4526 - acc: 0.7706Epoch 00038: val_loss improved from 0.40578 to 0.40449, saving model to Final038_0.79.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4518 - acc: 0.7716 - val_loss: 0.4045 - val_acc: 0.7920\n",
      "0.003\n",
      "Epoch 40/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4580 - acc: 0.7614Epoch 00039: val_loss improved from 0.40449 to 0.40347, saving model to Final039_0.79.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4579 - acc: 0.7618 - val_loss: 0.4035 - val_acc: 0.7934\n",
      "0.003\n",
      "Epoch 41/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4569 - acc: 0.7687Epoch 00040: val_loss improved from 0.40347 to 0.40242, saving model to Final040_0.79.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4570 - acc: 0.7691 - val_loss: 0.4024 - val_acc: 0.7948\n",
      "0.003\n",
      "Epoch 42/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4459 - acc: 0.7737Epoch 00041: val_loss improved from 0.40242 to 0.40069, saving model to Final041_0.79.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4456 - acc: 0.7734 - val_loss: 0.4007 - val_acc: 0.7905\n",
      "0.003\n",
      "Epoch 43/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4499 - acc: 0.7684Epoch 00042: val_loss improved from 0.40069 to 0.39944, saving model to Final042_0.79.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4494 - acc: 0.7688 - val_loss: 0.3994 - val_acc: 0.7948\n",
      "0.003\n",
      "Epoch 44/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4490 - acc: 0.7727Epoch 00043: val_loss improved from 0.39944 to 0.39796, saving model to Final043_0.80.h5\n",
      "6271/6271 [==============================] - 27s - loss: 0.4495 - acc: 0.7726 - val_loss: 0.3980 - val_acc: 0.7977\n",
      "0.003\n",
      "Epoch 45/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4435 - acc: 0.7755Epoch 00044: val_loss improved from 0.39796 to 0.39619, saving model to Final044_0.80.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4438 - acc: 0.7750 - val_loss: 0.3962 - val_acc: 0.7977\n",
      "0.003\n",
      "Epoch 46/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4487 - acc: 0.7722Epoch 00045: val_loss improved from 0.39619 to 0.39447, saving model to Final045_0.80.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4484 - acc: 0.7724 - val_loss: 0.3945 - val_acc: 0.8006\n",
      "0.003\n",
      "Epoch 47/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4439 - acc: 0.7772Epoch 00046: val_loss improved from 0.39447 to 0.39337, saving model to Final046_0.80.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4441 - acc: 0.7772 - val_loss: 0.3934 - val_acc: 0.7977\n",
      "0.003\n",
      "Epoch 48/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4478 - acc: 0.7713Epoch 00047: val_loss improved from 0.39337 to 0.39249, saving model to Final047_0.80.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4476 - acc: 0.7716 - val_loss: 0.3925 - val_acc: 0.7991\n",
      "0.003\n",
      "Epoch 49/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4482 - acc: 0.7779Epoch 00048: val_loss improved from 0.39249 to 0.39142, saving model to Final048_0.79.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4484 - acc: 0.7782 - val_loss: 0.3914 - val_acc: 0.7948\n",
      "0.003\n",
      "Epoch 50/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4437 - acc: 0.7775Epoch 00049: val_loss improved from 0.39142 to 0.39015, saving model to Final049_0.80.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4434 - acc: 0.7769 - val_loss: 0.3902 - val_acc: 0.8006\n",
      "0.003\n",
      "Epoch 51/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4411 - acc: 0.7816Epoch 00050: val_loss improved from 0.39015 to 0.38876, saving model to Final050_0.80.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4416 - acc: 0.7807 - val_loss: 0.3888 - val_acc: 0.8049\n",
      "0.003\n",
      "Epoch 52/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4458 - acc: 0.7751Epoch 00051: val_loss improved from 0.38876 to 0.38831, saving model to Final051_0.80.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4449 - acc: 0.7761 - val_loss: 0.3883 - val_acc: 0.8020\n",
      "0.003\n",
      "Epoch 53/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4401 - acc: 0.7782Epoch 00052: val_loss improved from 0.38831 to 0.38702, saving model to Final052_0.80.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4405 - acc: 0.7785 - val_loss: 0.3870 - val_acc: 0.8020\n",
      "0.003\n",
      "Epoch 54/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4321 - acc: 0.7899Epoch 00053: val_loss improved from 0.38702 to 0.38550, saving model to Final053_0.81.h5\n",
      "6271/6271 [==============================] - 27s - loss: 0.4314 - acc: 0.7906 - val_loss: 0.3855 - val_acc: 0.8135\n",
      "0.003\n",
      "Epoch 55/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4286 - acc: 0.7877Epoch 00054: val_loss improved from 0.38550 to 0.38437, saving model to Final054_0.82.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4291 - acc: 0.7879 - val_loss: 0.3844 - val_acc: 0.8178\n",
      "0.003\n",
      "Epoch 56/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4304 - acc: 0.7809Epoch 00055: val_loss improved from 0.38437 to 0.38261, saving model to Final055_0.82.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4314 - acc: 0.7804 - val_loss: 0.3826 - val_acc: 0.8178\n",
      "0.003\n",
      "Epoch 57/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4366 - acc: 0.7845Epoch 00056: val_loss improved from 0.38261 to 0.38163, saving model to Final056_0.82.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4376 - acc: 0.7833 - val_loss: 0.3816 - val_acc: 0.8164\n",
      "0.003\n",
      "Epoch 58/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4342 - acc: 0.7829Epoch 00057: val_loss improved from 0.38163 to 0.38042, saving model to Final057_0.82.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4346 - acc: 0.7825 - val_loss: 0.3804 - val_acc: 0.8192\n",
      "0.003\n",
      "Epoch 59/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4399 - acc: 0.7792Epoch 00058: val_loss improved from 0.38042 to 0.37965, saving model to Final058_0.82.h5\n",
      "6271/6271 [==============================] - 27s - loss: 0.4399 - acc: 0.7796 - val_loss: 0.3796 - val_acc: 0.8207\n",
      "0.0009\n",
      "Epoch 60/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4331 - acc: 0.7882Epoch 00059: val_loss improved from 0.37965 to 0.37930, saving model to Final059_0.82.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4329 - acc: 0.7881 - val_loss: 0.3793 - val_acc: 0.8178\n",
      "0.0009\n",
      "Epoch 61/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4366 - acc: 0.7841Epoch 00060: val_loss improved from 0.37930 to 0.37898, saving model to Final060_0.82.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4366 - acc: 0.7842 - val_loss: 0.3790 - val_acc: 0.8178\n",
      "0.0009\n",
      "Epoch 62/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4315 - acc: 0.7912Epoch 00061: val_loss improved from 0.37898 to 0.37854, saving model to Final061_0.82.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4308 - acc: 0.7916 - val_loss: 0.3785 - val_acc: 0.8178\n",
      "0.0009\n",
      "Epoch 63/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4264 - acc: 0.7930Epoch 00062: val_loss improved from 0.37854 to 0.37824, saving model to Final062_0.82.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4269 - acc: 0.7921 - val_loss: 0.3782 - val_acc: 0.8192\n",
      "0.0009\n",
      "Epoch 64/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4312 - acc: 0.7875Epoch 00063: val_loss improved from 0.37824 to 0.37807, saving model to Final063_0.82.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4308 - acc: 0.7881 - val_loss: 0.3781 - val_acc: 0.8192\n",
      "0.0009\n",
      "Epoch 65/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4226 - acc: 0.7972Epoch 00064: val_loss improved from 0.37807 to 0.37769, saving model to Final064_0.82.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4230 - acc: 0.7968 - val_loss: 0.3777 - val_acc: 0.8192\n",
      "0.0009\n",
      "Epoch 66/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4277 - acc: 0.7877Epoch 00065: val_loss improved from 0.37769 to 0.37740, saving model to Final065_0.82.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4272 - acc: 0.7882 - val_loss: 0.3774 - val_acc: 0.8192\n",
      "0.0009\n",
      "Epoch 67/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4308 - acc: 0.7932Epoch 00066: val_loss improved from 0.37740 to 0.37701, saving model to Final066_0.82.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4298 - acc: 0.7937 - val_loss: 0.3770 - val_acc: 0.8207\n",
      "0.0009\n",
      "Epoch 68/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4354 - acc: 0.7862Epoch 00067: val_loss improved from 0.37701 to 0.37701, saving model to Final067_0.82.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4348 - acc: 0.7865 - val_loss: 0.3770 - val_acc: 0.8192\n",
      "0.0009\n",
      "Epoch 69/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4280 - acc: 0.7912Epoch 00068: val_loss improved from 0.37701 to 0.37664, saving model to Final068_0.82.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4281 - acc: 0.7913 - val_loss: 0.3766 - val_acc: 0.8192\n",
      "0.0009\n",
      "Epoch 70/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4257 - acc: 0.7887Epoch 00069: val_loss improved from 0.37664 to 0.37623, saving model to Final069_0.82.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4258 - acc: 0.7895 - val_loss: 0.3762 - val_acc: 0.8207\n",
      "0.0009\n",
      "Epoch 71/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4222 - acc: 0.7937Epoch 00070: val_loss improved from 0.37623 to 0.37569, saving model to Final070_0.82.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4225 - acc: 0.7929 - val_loss: 0.3757 - val_acc: 0.8178\n",
      "0.0009\n",
      "Epoch 72/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4326 - acc: 0.7870Epoch 00071: val_loss improved from 0.37569 to 0.37548, saving model to Final071_0.82.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4322 - acc: 0.7870 - val_loss: 0.3755 - val_acc: 0.8178\n",
      "0.0009\n",
      "Epoch 73/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4255 - acc: 0.7882Epoch 00072: val_loss improved from 0.37548 to 0.37518, saving model to Final072_0.82.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4250 - acc: 0.7882 - val_loss: 0.3752 - val_acc: 0.8192\n",
      "0.0009\n",
      "Epoch 74/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4209 - acc: 0.7906Epoch 00073: val_loss improved from 0.37518 to 0.37475, saving model to Final073_0.82.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4205 - acc: 0.7905 - val_loss: 0.3747 - val_acc: 0.8207\n",
      "0.0009\n",
      "Epoch 75/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4301 - acc: 0.7874Epoch 00074: val_loss improved from 0.37475 to 0.37439, saving model to Final074_0.82.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4308 - acc: 0.7870 - val_loss: 0.3744 - val_acc: 0.8221\n",
      "0.0009\n",
      "Epoch 76/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4257 - acc: 0.7957Epoch 00075: val_loss improved from 0.37439 to 0.37411, saving model to Final075_0.82.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4267 - acc: 0.7954 - val_loss: 0.3741 - val_acc: 0.8221\n",
      "0.0009\n",
      "Epoch 77/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4208 - acc: 0.8027Epoch 00076: val_loss improved from 0.37411 to 0.37364, saving model to Final076_0.82.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4206 - acc: 0.8029 - val_loss: 0.3736 - val_acc: 0.8221\n",
      "0.0009\n",
      "Epoch 78/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4277 - acc: 0.7924Epoch 00077: val_loss improved from 0.37364 to 0.37350, saving model to Final077_0.82.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4275 - acc: 0.7924 - val_loss: 0.3735 - val_acc: 0.8250\n",
      "0.0009\n",
      "Epoch 79/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4271 - acc: 0.7954Epoch 00078: val_loss improved from 0.37350 to 0.37306, saving model to Final078_0.82.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4271 - acc: 0.7956 - val_loss: 0.3731 - val_acc: 0.8235\n",
      "0.0009\n",
      "Epoch 80/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4259 - acc: 0.7874Epoch 00079: val_loss improved from 0.37306 to 0.37262, saving model to Final079_0.82.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4264 - acc: 0.7866 - val_loss: 0.3726 - val_acc: 0.8221\n",
      "0.0009\n",
      "Epoch 81/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4206 - acc: 0.8004Epoch 00080: val_loss improved from 0.37262 to 0.37234, saving model to Final080_0.82.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4209 - acc: 0.8002 - val_loss: 0.3723 - val_acc: 0.8207\n",
      "0.0009\n",
      "Epoch 82/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4271 - acc: 0.7861Epoch 00081: val_loss improved from 0.37234 to 0.37207, saving model to Final081_0.82.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4274 - acc: 0.7862 - val_loss: 0.3721 - val_acc: 0.8221\n",
      "0.0009\n",
      "Epoch 83/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4278 - acc: 0.7882Epoch 00082: val_loss improved from 0.37207 to 0.37185, saving model to Final082_0.82.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4273 - acc: 0.7886 - val_loss: 0.3719 - val_acc: 0.8235\n",
      "0.0009\n",
      "Epoch 84/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4252 - acc: 0.7872Epoch 00083: val_loss improved from 0.37185 to 0.37174, saving model to Final083_0.82.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4248 - acc: 0.7878 - val_loss: 0.3717 - val_acc: 0.8221\n",
      "0.0009\n",
      "Epoch 85/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4176 - acc: 0.7956Epoch 00084: val_loss improved from 0.37174 to 0.37152, saving model to Final084_0.82.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4180 - acc: 0.7952 - val_loss: 0.3715 - val_acc: 0.8192\n",
      "0.0009\n",
      "Epoch 86/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4225 - acc: 0.7875Epoch 00085: val_loss improved from 0.37152 to 0.37095, saving model to Final085_0.82.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.4225 - acc: 0.7878 - val_loss: 0.3709 - val_acc: 0.8235\n",
      "0.0009\n",
      "Epoch 87/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4230 - acc: 0.7928Epoch 00086: val_loss improved from 0.37095 to 0.37074, saving model to Final086_0.82.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4239 - acc: 0.7921 - val_loss: 0.3707 - val_acc: 0.8235\n",
      "0.0009\n",
      "Epoch 88/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4248 - acc: 0.7935Epoch 00087: val_loss improved from 0.37074 to 0.37052, saving model to Final087_0.82.h5\n",
      "6271/6271 [==============================] - 27s - loss: 0.4242 - acc: 0.7941 - val_loss: 0.3705 - val_acc: 0.8221\n",
      "0.0009\n",
      "Epoch 89/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4201 - acc: 0.7991Epoch 00088: val_loss improved from 0.37052 to 0.37016, saving model to Final088_0.82.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4193 - acc: 0.7996 - val_loss: 0.3702 - val_acc: 0.8221\n",
      "0.00026999999999999995\n",
      "Epoch 90/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4248 - acc: 0.7966Epoch 00089: val_loss improved from 0.37016 to 0.37002, saving model to Final089_0.82.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4245 - acc: 0.7968 - val_loss: 0.3700 - val_acc: 0.8235\n",
      "0.00026999999999999995\n",
      "Epoch 91/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4251 - acc: 0.7933Epoch 00090: val_loss improved from 0.37002 to 0.36993, saving model to Final090_0.82.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4249 - acc: 0.7933 - val_loss: 0.3699 - val_acc: 0.8235\n",
      "0.00026999999999999995\n",
      "Epoch 92/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4180 - acc: 0.7983Epoch 00091: val_loss improved from 0.36993 to 0.36991, saving model to Final091_0.82.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4192 - acc: 0.7976 - val_loss: 0.3699 - val_acc: 0.8235\n",
      "0.00026999999999999995\n",
      "Epoch 93/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4275 - acc: 0.7867Epoch 00092: val_loss improved from 0.36991 to 0.36979, saving model to Final092_0.82.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4269 - acc: 0.7876 - val_loss: 0.3698 - val_acc: 0.8235\n",
      "0.00026999999999999995\n",
      "Epoch 94/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4205 - acc: 0.7930Epoch 00093: val_loss improved from 0.36979 to 0.36974, saving model to Final093_0.82.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4197 - acc: 0.7938 - val_loss: 0.3697 - val_acc: 0.8235\n",
      "0.00026999999999999995\n",
      "Epoch 95/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4232 - acc: 0.7887Epoch 00094: val_loss improved from 0.36974 to 0.36959, saving model to Final094_0.82.h5\n",
      "6271/6271 [==============================] - 27s - loss: 0.4240 - acc: 0.7887 - val_loss: 0.3696 - val_acc: 0.8235\n",
      "0.00026999999999999995\n",
      "Epoch 96/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4140 - acc: 0.7988Epoch 00095: val_loss improved from 0.36959 to 0.36947, saving model to Final095_0.82.h5\n",
      "6271/6271 [==============================] - 27s - loss: 0.4150 - acc: 0.7983 - val_loss: 0.3695 - val_acc: 0.8235\n",
      "0.00026999999999999995\n",
      "Epoch 97/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4205 - acc: 0.7956Epoch 00096: val_loss improved from 0.36947 to 0.36938, saving model to Final096_0.82.h5\n",
      "6271/6271 [==============================] - 27s - loss: 0.4204 - acc: 0.7960 - val_loss: 0.3694 - val_acc: 0.8250\n",
      "0.00026999999999999995\n",
      "Epoch 98/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4209 - acc: 0.7962Epoch 00097: val_loss improved from 0.36938 to 0.36935, saving model to Final097_0.82.h5\n",
      "6271/6271 [==============================] - 27s - loss: 0.4209 - acc: 0.7964 - val_loss: 0.3693 - val_acc: 0.8235\n",
      "0.00026999999999999995\n",
      "Epoch 99/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4216 - acc: 0.7932Epoch 00098: val_loss improved from 0.36935 to 0.36922, saving model to Final098_0.82.h5\n",
      "6271/6271 [==============================] - 27s - loss: 0.4220 - acc: 0.7924 - val_loss: 0.3692 - val_acc: 0.8235\n",
      "0.00026999999999999995\n",
      "Epoch 100/100\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.4227 - acc: 0.7977Epoch 00099: val_loss improved from 0.36922 to 0.36917, saving model to Final099_0.82.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.4219 - acc: 0.7980 - val_loss: 0.3692 - val_acc: 0.8250\n",
      "['__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_fit_loop', '_flattened_layers', '_gather_dict_attr', '_gather_list_attr', '_get_node_attribute_at_index', '_make_predict_function', '_make_test_function', '_make_train_function', '_output_mask_cache', '_output_shape_cache', '_output_tensor_cache', '_predict_loop', '_standardize_user_data', '_test_loop', '_updated_config', 'add', 'add_inbound_node', 'assert_input_compatibility', 'build', 'built', 'call', 'compile', 'compute_mask', 'constraints', 'container_nodes', 'count_params', 'create_input_layer', 'evaluate', 'evaluate_generator', 'fit', 'fit_generator', 'flattened_layers', 'from_config', 'get_config', 'get_input_at', 'get_input_mask_at', 'get_input_shape_at', 'get_layer', 'get_output_at', 'get_output_mask_at', 'get_output_shape_at', 'get_output_shape_for', 'get_weights', 'inbound_nodes', 'input', 'input_layers', 'input_layers_node_indices', 'input_layers_tensor_indices', 'input_mask', 'input_names', 'input_shape', 'input_spec', 'inputs', 'layers', 'load_weights', 'load_weights_from_hdf5_group', 'load_weights_from_hdf5_group_by_name', 'loss', 'loss_weights', 'metrics', 'metrics_names', 'metrics_tensors', 'model', 'name', 'nodes_by_depth', 'non_trainable_weights', 'optimizer', 'outbound_nodes', 'output', 'output_layers', 'output_layers_node_indices', 'output_layers_tensor_indices', 'output_mask', 'output_names', 'output_shape', 'outputs', 'pop', 'predict', 'predict_classes', 'predict_generator', 'predict_on_batch', 'predict_proba', 'regularizers', 'reset_states', 'run_internal_graph', 'sample_weight_mode', 'save', 'save_weights', 'save_weights_to_hdf5_group', 'set_input', 'set_weights', 'state_updates', 'stateful', 'stop_training', 'summary', 'supports_masking', 'test_on_batch', 'to_json', 'to_yaml', 'train_on_batch', 'trainable', 'trainable_weights', 'training_data', 'updates', 'uses_learning_phase', 'validation_data', 'weights']\n",
      "ROC: 0.92090159081\n",
      "[[ 0.3341324 ]\n",
      " [ 0.98284125]\n",
      " [ 0.9351663 ]\n",
      " ..., \n",
      " [ 0.89212221]\n",
      " [ 0.91738772]\n",
      " [ 0.45692936]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0     0.7876    0.5880    0.6733       517\n",
      "    class 1     0.8429    0.9331    0.8857      1225\n",
      "\n",
      "avg / total     0.8265    0.8307    0.8227      1742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    trainandpredict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
