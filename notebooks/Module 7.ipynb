{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple binary classification problem utilizing convolutional neural networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 750 Ti (CNMeM is disabled, cuDNN 5105)\n",
      "/usr/local/lib/python3.5/dist-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries. \n",
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "# os.environ['THEANO_FLAGS']='mode=FAST_RUN,device=gpu0,floatX=float32,optimizer=fast_compile'\n",
    "# os.environ['KERAS_BACKEND'] = 'theano'\n",
    "\"\"\"\n",
    "os.environ['THEANO_FLAGS']='mode=FAST_RUN,device=gpu3,floatX=float32,optimizer=fast_compile'\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "\n",
    "In case you want to select a graphic card (i the above code i set the 3rd graphic card.) \n",
    "\"\"\"\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import numpy as np\n",
    "import keras \n",
    "import keras.backend as K\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "from keras import callbacks\n",
    "import glob\n",
    "from PIL import Image\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.utils.visualize_util import plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6241\n",
      "6241\n"
     ]
    }
   ],
   "source": [
    "# It is good to know the pid of the running code in case you need to stop  or monitor. \n",
    "print (os.getpid())\n",
    "file_open = lambda x,y: glob.glob(os.path.join(x,y))\n",
    "\n",
    "# learning rate schedule. It is helpful when the learning rate can be dynamically set up. We will be using the callback functionality that keras provides. \n",
    "def step_decay(epoch):\n",
    "  initial_lrate =0.01\n",
    "  drop = 0.3\n",
    "  epochs_drop = 30.0\n",
    "  lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "  print (lrate)\n",
    "  return lrate\n",
    "\n",
    "# The following function will be used to give a number of the parameters in our model. Useful when we need to get an estimate of what size of dataset we have to use.  \n",
    "def size(model): \n",
    "  return sum([np.prod(K.get_value(w).shape) for w in model.trainable_weights])\n",
    "\n",
    "def createmodel(img_channels,img_rows,img_cols):\n",
    "  # This is a Sequential model. Graph models can be used in order to create more complex networks. \n",
    "  # Teaching Points:\n",
    "  # 1. Here we utilize the adam optimization algorithm. In order to use the SGD algorithm one could replace the {adam=keras.optimizers.Adadelta(lr=0)} line with  {sgd = SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False)} make sure you import the correct optimizer from keras. \n",
    "  # 2. This is a binary classification problem so make sure that the correct activation loss function combination is used. For such a problem the sigmoid activation function with the binary cross entropy loss is a good option\n",
    "  # 3. Since this is a binary problem use   model.add(Dense(1)) NOT 2...\n",
    "  # 4. For multi class model this code can be easily modified by selecting the softmax as activation function and the categorical cross entropy as loss \n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(Convolution2D(16, 3, 3, border_mode='same',input_shape=(1, img_rows, img_cols)))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Convolution2D(16, 5, 5, border_mode='same'))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Convolution2D(32, 3, 3, border_mode='same'))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Convolution2D(64, 5, 5, border_mode='same'))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Convolution2D(128, 3, 3, border_mode='same'))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(128, init='he_normal'))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.5)) \n",
    "  model.add(Dense(32, init='he_normal'))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.5)) \n",
    "  model.add(Dense(1))\n",
    "  # model.add(Activation('relu'))\n",
    "  model.add(Activation('sigmoid'))\n",
    "  # learning schedule callback\n",
    "  adam=keras.optimizers.Adadelta(lr=0)\n",
    "  lrate = LearningRateScheduler(step_decay)\n",
    "  model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "  plot(model, to_file='model.png')\n",
    "  return model\n",
    "\n",
    "def shuffle(X, y):\n",
    "  perm = np.random.permutation(len(y))\n",
    "  X = X[perm]\n",
    "  y = y[perm]\n",
    "  print (np.shape(X))\n",
    "  return X, y\n",
    "\n",
    "\n",
    "def read_data(image):\n",
    "  \"opens image and converts it to a m*n matrix\" \n",
    "  image = Image.open(image)\n",
    "  image = image.getdata()\n",
    "  # image = list(image.getdata())\n",
    "  # image = map(list,image)\n",
    "  image = np.array(image)\n",
    "  return image.reshape(-1)\n",
    "\n",
    "def createTrainTestValset(image_dir1, image_dir2):\n",
    "  Class1_images = file_open(image_dir1,\"*.jpg\")\n",
    "  Class2_images = file_open(image_dir2,\"*.jpg\")\n",
    "  Class1_set = []\n",
    "  Class2_set = []\n",
    "  # Read all the files, and create numpy arrays. \n",
    "  Class1_set = [read_data(image) for image in Class1_images]\n",
    "  Class2_set = [read_data(image) for image in Class2_images]\n",
    "  Class1_set = np.array(Class1_set) #This is where the Memory Error occurs\n",
    "  Class2_set = np.array(Class2_set)\n",
    "  X=np.vstack((Class1_set, Class2_set))\n",
    "  X=X.astype(np.float)/255\n",
    "  # print (np.shape(X))\n",
    "  yclass1=np.zeros((np.shape(Class1_set)[0]))\n",
    "  yclass2=np.ones((np.shape(Class2_set)[0]))\n",
    "  # print (np.shape(yclass1))\n",
    "  y=np.concatenate((yclass1, yclass2))\n",
    "  # print (np.shape(y)) \n",
    "  X,y=shuffle(X, y)\n",
    "  print (np.shape(X)) \n",
    "  print (np.max(X))\n",
    "  print (np.shape(y)) \n",
    "  X_train, X_val,y_train, y_val= train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "  return X_train,y_train, X_val, y_val \n",
    "\n",
    "  # Read the images; and split them in three different sets. \n",
    "def trainandpredict(Scan=32 ,img_channels=1,batch_size=64,nb_epoch=5,data_augmentation=False):\n",
    "  img_rows=Scan\n",
    "  img_cols=Scan\n",
    "  CurrentDir= os.getcwd()\n",
    "  image_dir1=os.path.abspath(os.path.join(os.path.abspath(os.path.join(CurrentDir, os.pardir)), \"Data\",\"negative_images\"))\n",
    "#   print(image_dir1)\n",
    "  image_dir2=os.path.abspath(os.path.join(os.path.abspath(os.path.join(CurrentDir, os.pardir)), \"Data\",\"positive_images\"))\n",
    "  modeleval=createmodel(img_channels,img_rows,img_cols)\n",
    "  X_train,y_train, X_val, y_val = createTrainTestValset(image_dir1, image_dir2)\n",
    "  X_train =X_train.reshape(\n",
    "    -1,  # number of samples, -1 makes it so that this number is determined automatically\n",
    "    1,   # 1 color channel, since images are only black and white\n",
    "    Scan,  # first image dimension (vertical)\n",
    "    Scan,  # second image dimension (horizontal)\n",
    "  )\n",
    "  X_val =X_val.reshape(\n",
    "    -1,  # number of samples, -1 makes it so that this number is determined automatically\n",
    "    1,   # 1 color channel, since images are only black and white\n",
    "    Scan,  # first image dimension (vertical)\n",
    "    Scan,  # second image dimension (horizontal)\n",
    "  )\n",
    "  # Callbacks\n",
    "  best_model = ModelCheckpoint('Final.h5', verbose=1, monitor='val_loss',save_best_only=True)\n",
    "  lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "  # Data augmentation is always a good choice\n",
    "  if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    modeleval.load_weights('Final.h5')\n",
    "    modeleval.fit(X_train, y_train,batch_size=batch_size,nb_epoch=nb_epoch,validation_split=0.1,callbacks=[best_model,lrate],shuffle=True)\n",
    "  else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    print (\"pending\")\n",
    "    sys.exit()\n",
    "    # this will do preprocessing and realtime data augmentation\n",
    "    # datagen = ImageDataGenerator(\n",
    "    #   featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    #   samplewise_center=False,  # set each sample mean to 0\n",
    "    #   featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    #   samplewise_std_normalization=False,  # divide each input by its std\n",
    "    #   zca_whitening=False,  # apply ZCA whitening\n",
    "    #   rotation_range=3,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    #   width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "    #   height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "    #   horizontal_flip=True,  # randomly flip images\n",
    "    #   vertical_flip=False)  # randomly flip images\n",
    "    # modeleval.fit(X_train, y_train,batch_size=batch_size,nb_epoch=nb_epoch,validation_data=(X_train1, y_train1),callbacks=[best_model,lrate],shuffle=True)\n",
    "\n",
    "\n",
    "  # Some evaluation Just the basic stuff... \n",
    "  print (dir(modeleval))\n",
    "  Y_cv_pred = modeleval.predict(X_val, batch_size = 32)\n",
    "  roc =roc_auc_score(y_val, Y_cv_pred)\n",
    "  print(\"ROC:\", roc)\n",
    "  print (Y_cv_pred)\n",
    "  Y_cv_pred[Y_cv_pred>=.5]=1\n",
    "  Y_cv_pred[Y_cv_pred<.5]=0\n",
    "  target_names=[] \n",
    "  # print (\"The f1-score gives you the harmonic mean of precision and recall. The scores corresponding to every class will tell you the accuracy of the classifier in classifying the data points in that particular class compared to all other classes.The support is the number of samples of the true response that lie in that class.\")\n",
    "  target_names = ['class 0', 'class 1']\n",
    "  print(classification_report(y_val, Y_cv_pred, target_names=target_names,digits=4))\n",
    "\n",
    "# It is good to know the pid of the running code in case you need to stop  or monitor. \n",
    "print (os.getpid())\n",
    "file_open = lambda x,y: glob.glob(os.path.join(x,y))\n",
    "\n",
    "# learning rate schedule. It is helpful when the learning rate can be dynamically set up. We will be using the callback functionality that keras provides. \n",
    "def step_decay(epoch):\n",
    "  initial_lrate =0.01\n",
    "  drop = 0.3\n",
    "  epochs_drop = 30.0\n",
    "  lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "  print (lrate)\n",
    "  return lrate\n",
    "\n",
    "# The following function will be used to give a number of the parameters in our model. Useful when we need to get an estimate of what size of dataset we have to use.  \n",
    "def size(model): \n",
    "  return sum([np.prod(K.get_value(w).shape) for w in model.trainable_weights])\n",
    "\n",
    "def createmodel(img_channels,img_rows,img_cols):\n",
    "  # This is a Sequential model. Graph models can be used in order to create more complex networks. \n",
    "  # Teaching Points:\n",
    "  # 1. Here we utilize the adam optimization algorithm. In order to use the SGD algorithm one could replace the {adam=keras.optimizers.Adadelta(lr=0)} line with  {sgd = SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False)} make sure you import the correct optimizer from keras. \n",
    "  # 2. This is a binary classification problem so make sure that the correct activation loss function combination is used. For such a problem the sigmoid activation function with the binary cross entropy loss is a good option\n",
    "  # 3. Since this is a binary problem use   model.add(Dense(1)) NOT 2...\n",
    "  # 4. For multi class model this code can be easily modified by selecting the softmax as activation function and the categorical cross entropy as loss \n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(Convolution2D(16, 3, 3, border_mode='same',input_shape=(1, img_rows, img_cols)))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Convolution2D(16, 5, 5, border_mode='same'))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Convolution2D(32, 3, 3, border_mode='same'))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Convolution2D(64, 5, 5, border_mode='same'))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Convolution2D(128, 3, 3, border_mode='same'))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(128, init='he_normal'))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.5)) \n",
    "  model.add(Dense(32, init='he_normal'))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.5)) \n",
    "  model.add(Dense(1))\n",
    "  # model.add(Activation('relu'))\n",
    "  model.add(Activation('sigmoid'))\n",
    "  # learning schedule callback\n",
    "  adam=keras.optimizers.Adadelta(lr=0)\n",
    "  lrate = LearningRateScheduler(step_decay)\n",
    "  model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "  plot(model, to_file='model.png')\n",
    "  return model\n",
    "\n",
    "def shuffle(X, y):\n",
    "  perm = np.random.permutation(len(y))\n",
    "  X = X[perm]\n",
    "  y = y[perm]\n",
    "  print (np.shape(X))\n",
    "  return X, y\n",
    "\n",
    "\n",
    "def read_data(image):\n",
    "  \"opens image and converts it to a m*n matrix\" \n",
    "  image = Image.open(image)\n",
    "  image = image.getdata()\n",
    "  # image = list(image.getdata())\n",
    "  # image = map(list,image)\n",
    "  image = np.array(image)\n",
    "  return image.reshape(-1)\n",
    "\n",
    "def createTrainTestValset(image_dir1, image_dir2):\n",
    "  Class1_images = file_open(image_dir1,\"*.jpg\")\n",
    "  Class2_images = file_open(image_dir2,\"*.jpg\")\n",
    "  Class1_set = []\n",
    "  Class2_set = []\n",
    "  # Read all the files, and create numpy arrays. \n",
    "  Class1_set = [read_data(image) for image in Class1_images]\n",
    "  Class2_set = [read_data(image) for image in Class2_images]\n",
    "  Class1_set = np.array(Class1_set) #This is where the Memory Error occurs\n",
    "  Class2_set = np.array(Class2_set)\n",
    "  X=np.vstack((Class1_set, Class2_set))\n",
    "  X=X.astype(np.float)/255\n",
    "  # print (np.shape(X))\n",
    "  yclass1=np.zeros((np.shape(Class1_set)[0]))\n",
    "  yclass2=np.ones((np.shape(Class2_set)[0]))\n",
    "  # print (np.shape(yclass1))\n",
    "  y=np.concatenate((yclass1, yclass2))\n",
    "  # print (np.shape(y)) \n",
    "  X,y=shuffle(X, y)\n",
    "  print (np.shape(X)) \n",
    "  print (np.max(X))\n",
    "  print (np.shape(y)) \n",
    "  X_train, X_val,y_train, y_val= train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "  return X_train,y_train, X_val, y_val \n",
    "\n",
    "  # Read the images; and split them in three different sets. \n",
    "def trainandpredict(Scan=32 ,img_channels=1,batch_size=64,nb_epoch=5,data_augmentation=False):\n",
    "  img_rows=Scan\n",
    "  img_cols=Scan\n",
    "  CurrentDir= os.getcwd()\n",
    "  image_dir1=os.path.abspath(os.path.join(os.path.abspath(os.path.join(CurrentDir, os.pardir)), \"Data\",\"negative_images\"))\n",
    "#   print(image_dir1)\n",
    "  image_dir2=os.path.abspath(os.path.join(os.path.abspath(os.path.join(CurrentDir, os.pardir)), \"Data\",\"positive_images\"))\n",
    "  modeleval=createmodel(img_channels,img_rows,img_cols)\n",
    "  X_train,y_train, X_val, y_val = createTrainTestValset(image_dir1, image_dir2)\n",
    "  X_train =X_train.reshape(\n",
    "    -1,  # number of samples, -1 makes it so that this number is determined automatically\n",
    "    1,   # 1 color channel, since images are only black and white\n",
    "    Scan,  # first image dimension (vertical)\n",
    "    Scan,  # second image dimension (horizontal)\n",
    "  )\n",
    "  X_val =X_val.reshape(\n",
    "    -1,  # number of samples, -1 makes it so that this number is determined automatically\n",
    "    1,   # 1 color channel, since images are only black and white\n",
    "    Scan,  # first image dimension (vertical)\n",
    "    Scan,  # second image dimension (horizontal)\n",
    "  )\n",
    "  # Callbacks\n",
    "  best_model = ModelCheckpoint('Final.h5', verbose=1, monitor='val_loss',save_best_only=True)\n",
    "  lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "  # Data augmentation is always a good choice\n",
    "  if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    modeleval.load_weights('Final.h5')\n",
    "    modeleval.fit(X_train, y_train,batch_size=batch_size,nb_epoch=nb_epoch,validation_split=0.1,callbacks=[best_model,lrate],shuffle=True)\n",
    "  else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    print (\"pending\")\n",
    "    sys.exit()\n",
    "    # this will do preprocessing and realtime data augmentation\n",
    "    # datagen = ImageDataGenerator(\n",
    "    #   featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    #   samplewise_center=False,  # set each sample mean to 0\n",
    "    #   featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    #   samplewise_std_normalization=False,  # divide each input by its std\n",
    "    #   zca_whitening=False,  # apply ZCA whitening\n",
    "    #   rotation_range=3,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    #   width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "    #   height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "    #   horizontal_flip=True,  # randomly flip images\n",
    "    #   vertical_flip=False)  # randomly flip images\n",
    "    # modeleval.fit(X_train, y_train,batch_size=batch_size,nb_epoch=nb_epoch,validation_data=(X_train1, y_train1),callbacks=[best_model,lrate],shuffle=True)\n",
    "\n",
    "\n",
    "  # Some evaluation Just the basic stuff... \n",
    "  print (dir(modeleval))\n",
    "  Y_cv_pred = modeleval.predict(X_val, batch_size = 32)\n",
    "  roc =roc_auc_score(y_val, Y_cv_pred)\n",
    "  print(\"ROC:\", roc)\n",
    "  print (Y_cv_pred)\n",
    "  Y_cv_pred[Y_cv_pred>=.5]=1\n",
    "  Y_cv_pred[Y_cv_pred<.5]=0\n",
    "  target_names=[] \n",
    "  # print (\"The f1-score gives you the harmonic mean of precision and recall. The scores corresponding to every class will tell you the accuracy of the classifier in classifying the data points in that particular class compared to all other classes.The support is the number of samples of the true response that lie in that class.\")\n",
    "  target_names = ['class 0', 'class 1']\n",
    "  print(classification_report(y_val, Y_cv_pred, target_names=target_names,digits=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Program "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8710, 1024)\n",
      "(8710, 1024)\n",
      "1.0\n",
      "(8710,)\n",
      "Not using data augmentation.\n",
      "Train on 6271 samples, validate on 697 samples\n",
      "0.01\n",
      "Epoch 1/5\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.3776 - acc: 0.8225Epoch 00000: val_loss improved from inf to 0.32175, saving model to Final.h5\n",
      "6271/6271 [==============================] - 25s - loss: 0.3773 - acc: 0.8222 - val_loss: 0.3218 - val_acc: 0.8809\n",
      "0.01\n",
      "Epoch 2/5\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.3645 - acc: 0.8328Epoch 00001: val_loss improved from 0.32175 to 0.31808, saving model to Final.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.3644 - acc: 0.8332 - val_loss: 0.3181 - val_acc: 0.8766\n",
      "0.01\n",
      "Epoch 3/5\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.3629 - acc: 0.8307Epoch 00002: val_loss improved from 0.31808 to 0.31692, saving model to Final.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.3635 - acc: 0.8302 - val_loss: 0.3169 - val_acc: 0.8809\n",
      "0.01\n",
      "Epoch 4/5\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.3624 - acc: 0.8313Epoch 00003: val_loss improved from 0.31692 to 0.31579, saving model to Final.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.3629 - acc: 0.8305 - val_loss: 0.3158 - val_acc: 0.8852\n",
      "0.01\n",
      "Epoch 5/5\n",
      "6208/6271 [============================>.] - ETA: 0s - loss: 0.3578 - acc: 0.8384Epoch 00004: val_loss improved from 0.31579 to 0.31230, saving model to Final.h5\n",
      "6271/6271 [==============================] - 26s - loss: 0.3574 - acc: 0.8385 - val_loss: 0.3123 - val_acc: 0.8824\n",
      "['__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_fit_loop', '_flattened_layers', '_gather_dict_attr', '_gather_list_attr', '_get_node_attribute_at_index', '_make_predict_function', '_make_test_function', '_make_train_function', '_output_mask_cache', '_output_shape_cache', '_output_tensor_cache', '_predict_loop', '_standardize_user_data', '_test_loop', '_updated_config', 'add', 'add_inbound_node', 'assert_input_compatibility', 'build', 'built', 'call', 'compile', 'compute_mask', 'constraints', 'container_nodes', 'count_params', 'create_input_layer', 'evaluate', 'evaluate_generator', 'fit', 'fit_generator', 'flattened_layers', 'from_config', 'get_config', 'get_input_at', 'get_input_mask_at', 'get_input_shape_at', 'get_layer', 'get_output_at', 'get_output_mask_at', 'get_output_shape_at', 'get_output_shape_for', 'get_weights', 'inbound_nodes', 'input', 'input_layers', 'input_layers_node_indices', 'input_layers_tensor_indices', 'input_mask', 'input_names', 'input_shape', 'input_spec', 'inputs', 'layers', 'load_weights', 'load_weights_from_hdf5_group', 'load_weights_from_hdf5_group_by_name', 'loss', 'loss_weights', 'metrics', 'metrics_names', 'metrics_tensors', 'model', 'name', 'nodes_by_depth', 'non_trainable_weights', 'optimizer', 'outbound_nodes', 'output', 'output_layers', 'output_layers_node_indices', 'output_layers_tensor_indices', 'output_mask', 'output_names', 'output_shape', 'outputs', 'pop', 'predict', 'predict_classes', 'predict_generator', 'predict_on_batch', 'predict_proba', 'regularizers', 'reset_states', 'run_internal_graph', 'sample_weight_mode', 'save', 'save_weights', 'save_weights_to_hdf5_group', 'set_input', 'set_weights', 'state_updates', 'stateful', 'stop_training', 'summary', 'supports_masking', 'test_on_batch', 'to_json', 'to_yaml', 'train_on_batch', 'trainable', 'trainable_weights', 'training_data', 'updates', 'uses_learning_phase', 'validation_data', 'weights']\n",
      "ROC: 0.936208267091\n",
      "[[ 0.37849411]\n",
      " [ 0.24506178]\n",
      " [ 0.98440951]\n",
      " ..., \n",
      " [ 0.54057229]\n",
      " [ 0.51199996]\n",
      " [ 0.76022846]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0     0.7882    0.7761    0.7821       518\n",
      "    class 1     0.9058    0.9118    0.9088      1224\n",
      "\n",
      "avg / total     0.8709    0.8714    0.8711      1742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    trainandpredict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
